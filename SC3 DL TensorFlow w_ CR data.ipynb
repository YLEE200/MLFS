{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FOR FINANCIAL SERVICES\n",
    "\n",
    "Welcome to IBM's Data Science Experience! This exciting tool will help your life a lot easier as a data scientist. Below is a simple introductory example of how easy for you to load your data and run a deep learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Default Prediction with Deep Learning \n",
    "In this notebook, I used a sample credit card data to predict write-off prediction using TesorFlow-based deep learning technique.  Deep-learning is gaining great momentum these days as those enable users to tackle various modeling challenges that we were not able to easily address such as image recognition.  Yet, there are many applicable techniques for financial services and I hope you can see how easy it is to adopt this deep learning technique for a real business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "> from [wikipedia](https://en.wikipedia.org/wiki/TensorFlow)\n",
    "\n",
    "TensorFlow™ is an open source software library for numerical computation using data flow graphs. TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on 9 November 2015. The name of the library help us understand how we work with it. Basically, tensors are multidimensional arrays that flow through the nodes of a graph.\n",
    "\n",
    "In the data flow graphs, nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n",
    "\n",
    "In this notebook, a simple illustrative example of MLP (multi-layer perceptrons) with two hidden layers, which therefore the model estimation is rather simple, is to be modeled via TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Neural Network Architecture \n",
    "\n",
    "The neural network will have 2 hidden layers (you have to choose how many hidden layers the network will have, is part of the architecture design). The job of each hidden layer is to transform the inputs into something that the output layer can use. Each node (neuron) is multiplied by a weight. Every node has a weight value, and during the training phase the neural network adjusts these values in order to produce a correct output. In addition to multiplying each input node by a weight, the network also adds a bias (role of bias in neural networks).\n",
    "\n",
    "In your architecture after multiplying the inputs by the weights and sum the values to the bias, the data also pass by an activation function. This activation function defines the final output of each node. An analogy: imagine that each node is a lamp, the activation function tells if the lamp will light or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load the data:\n",
    "\n",
    "1. Load your local file into your notebook. Click the **Find and Add Data** icon on the notebook action bar. Drop the file into the box or browse to select the file. The file is loaded to your object storage and appears in the Data Assets section of the project. For more information, see <a href=\"https://datascience.ibm.com/docs/content/analyze-data/load-and-access-data.html\" target=\"_blank\" rel=\"noopener noreferrer\">Load and access data</a>.\n",
    "1. click in the next code cell and select **Insert to code > pandas DataFrame** under the file name.\n",
    "\n",
    "For this exercise, the above steps were done in advance, and a csv file has been put in this notebook's working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & Check the data dimensions\n",
    "Deep learning techniques which are designed to handle computationally expensive algorithms take to the data into numerical dimensions so that the model layers can perform and estimate inputs and outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 8)\n",
      "(610,)\n"
     ]
    }
   ],
   "source": [
    "# Read the input data, pre-process and test-check on the dimensions of X and y\n",
    "\n",
    "df = pd.read_csv('CRPMT_SAMPLE.csv')\n",
    "df['PROD_NO'] = df.PROD.map({'1.REG': 0, '2.GOLD':1, '3.PLAT':2})\n",
    "\n",
    "feature_cols = [          \n",
    "        'CURR_BAL',                                               \n",
    "        'TENURE',                       \n",
    "        'CUST_INC',                      \n",
    "        'CUST_AGE',                                \n",
    "        'PMT_DUE',                                               \n",
    "        'NO_DM_CNT',               \n",
    "        'FICO_SCR',\n",
    "        'PROD_NO'\n",
    "    ]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df.WRITE_OFF_IND\n",
    "\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CURR_BAL</th>\n",
       "      <th>TENURE</th>\n",
       "      <th>CUST_INC</th>\n",
       "      <th>CUST_AGE</th>\n",
       "      <th>PMT_DUE</th>\n",
       "      <th>NO_DM_CNT</th>\n",
       "      <th>FICO_SCR</th>\n",
       "      <th>PROD_NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>755.16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44212</td>\n",
       "      <td>46</td>\n",
       "      <td>60.41</td>\n",
       "      <td>5</td>\n",
       "      <td>651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276.61</td>\n",
       "      <td>0.7</td>\n",
       "      <td>86249</td>\n",
       "      <td>34</td>\n",
       "      <td>22.13</td>\n",
       "      <td>10</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424.70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>79474</td>\n",
       "      <td>45</td>\n",
       "      <td>21.23</td>\n",
       "      <td>22</td>\n",
       "      <td>753</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CURR_BAL  TENURE  CUST_INC  CUST_AGE  PMT_DUE  NO_DM_CNT  FICO_SCR  PROD_NO\n",
       "0    755.16     3.0     44212        46    60.41          5       651        0\n",
       "1    276.61     0.7     86249        34    22.13         10       702        0\n",
       "2    424.70     0.1     79474        45    21.23         22       753        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the model later on\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "Name: WRITE_OFF_IND, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the model later on\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Format the data into dimensional matrices\n",
    "As mentioned above, TensorFlow, just like the other typical neural networks, is taking multi-dimensional data arrays (tensors) that flow through the nodes of a data flow graph.  Hence it is important to define (or set up) the layers (input, hidden and output) with the proper array dimensions.  In this example, we have (610,8) input layer and (610,1) output layer matrices, which will be re-arrayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.55160000e+02   3.00000000e+00   4.42120000e+04 ...,   5.00000000e+00\n",
      "    6.51000000e+02   0.00000000e+00]\n",
      " [  2.76610000e+02   7.00000000e-01   8.62490000e+04 ...,   1.00000000e+01\n",
      "    7.02000000e+02   0.00000000e+00]\n",
      " [  4.24700000e+02   1.00000000e-01   7.94740000e+04 ...,   2.20000000e+01\n",
      "    7.53000000e+02   1.00000000e+00]\n",
      " ..., \n",
      " [  5.85830000e+02   1.90000000e+00   6.49210000e+04 ...,   4.00000000e+00\n",
      "    6.50000000e+02   0.00000000e+00]\n",
      " [  2.69950000e+02   1.60000000e+00   5.73540000e+04 ...,   1.60000000e+01\n",
      "    7.68000000e+02   1.00000000e+00]\n",
      " [  6.91900000e+02   6.00000000e+00   8.37460000e+04 ...,   6.00000000e+00\n",
      "    6.56000000e+02   0.00000000e+00]]\n",
      "     0  1\n",
      "0    1  0\n",
      "1    0  1\n",
      "2    0  1\n",
      "3    0  1\n",
      "4    1  0\n",
      "5    0  1\n",
      "6    0  1\n",
      "7    0  1\n",
      "8    1  0\n",
      "9    0  1\n",
      "10   1  0\n",
      "11   0  1\n",
      "12   0  1\n",
      "13   1  0\n",
      "14   0  1\n",
      "15   0  1\n",
      "16   1  0\n",
      "17   0  1\n",
      "18   1  0\n",
      "19   0  1\n",
      "20   0  1\n",
      "21   0  1\n",
      "22   0  1\n",
      "23   0  1\n",
      "24   0  1\n",
      "25   0  1\n",
      "26   0  1\n",
      "27   0  1\n",
      "28   0  1\n",
      "29   0  1\n",
      "..  .. ..\n",
      "580  1  0\n",
      "581  0  1\n",
      "582  0  1\n",
      "583  0  1\n",
      "584  0  1\n",
      "585  0  1\n",
      "586  1  0\n",
      "587  0  1\n",
      "588  0  1\n",
      "589  0  1\n",
      "590  0  1\n",
      "591  0  1\n",
      "592  0  1\n",
      "593  0  1\n",
      "594  1  0\n",
      "595  0  1\n",
      "596  0  1\n",
      "597  0  1\n",
      "598  1  0\n",
      "599  0  1\n",
      "600  0  1\n",
      "601  0  1\n",
      "602  0  1\n",
      "603  0  1\n",
      "604  0  1\n",
      "605  0  1\n",
      "606  0  1\n",
      "607  0  1\n",
      "608  0  1\n",
      "609  0  1\n",
      "\n",
      "[610 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "x_labels = np.array(X.values)\n",
    "y_label = y.values\n",
    "\n",
    "y = []\n",
    "value = []\n",
    "for i in y_label:\n",
    "    if i not in value:\n",
    "        value.append(i)\n",
    "\n",
    "for i in y_label:\n",
    "    y.append(value.index(i))\n",
    "\n",
    "y_label = pd.get_dummies(y)\n",
    "\n",
    "print (x_labels)\n",
    "print (y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting parameters: you could change these based on your data structure\n",
    "# you need to reduce dimensions as the process flows from input to output\n",
    "\n",
    "n_input = 8 # input features\n",
    "n_hidden_1 = 5 # 1st layer's number of features; pick a # that may be a mid-point between prior and next layers\n",
    "n_hidden_2 = 3 # 2nd layer's number of features; pick a # that may be a mid-point between prior and next layers\n",
    "n_classes = 2 # total output classes : 0/1 binary array: non write-off (1,0) vs. write-off(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph set up\n",
    "\n",
    "# creating a palceholder for the holding the input value data that will be fed during run time \n",
    "# number of rows undefined to get any values, 8 is set for the number of columns\n",
    "X_input = tf.placeholder(tf.float32, [None, n_input])\n",
    "\n",
    "# creating a placeholder for holding the actual y data that will fed during the run time\n",
    "# number of rows undefined to get any values, 2 is set for the number of columns\n",
    "y_output = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Weights & Biases\n",
    "\n",
    "In neural networks, it is important to understand weights and biases (not just for the coding purpose).  In a simplest concept, you may want to think about a simple linear function expressed in Y = mX + c, where m is for slope and c is for constant.  Then, simply replace that with Y = wX + b, where w is a matrix of weights and b is a bias. Think of the weights as importance you assign to each feature (row of an input). Bias is independent of features. If the bias for that row (category) is high, then the score for that row will be higher.\n",
    "                    \n",
    "In TensorFlow, the weights are initialized with the tf.random_normal and given their shape of a 2-D tensor with the first dim representing the number of units in the layer from which the weights connect and the second dim representing the number of units in the layer to which the weights connect. The tf.random_normal initializer generates a random distribution with a given mean and standard deviation.  As you see below, the dimensions become reduced as the data/flow progresses. Often the features in the hidden layers are not easily interpretable by humans. \n",
    "\n",
    "This is a type of dimensionality reduction (or a type of feature extraction) as the next layer reduces its dimension from the prior dimensions by taking combinatorial dimensional values.  To simply put, overall neural networks is designed to extract the first layer or input data (a more complex matrix of data) and process to reduce (or extract) the complexity into a simpler output (or a target vector) so that the process is modeled to predict the target value. \n",
    "\n",
    "Then the biases are initialized with tf.zeros to ensure they start with all zero values, and their shape is simply the number of units in the layer to which they connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting layers' weights & biases\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
    "w2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "out_w =  tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    " \n",
    "b1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "b2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "out_b = tf.Variable(tf.zeros([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ReLU activation?\n",
    "\n",
    "In the context of artificial neural networks, the rectifier is an activation function defined as:\n",
    "\n",
    "f(x) = max (0,x), where x is the input to a neuron. \n",
    "\n",
    "The important thing to note is that it’s non-linear (as opposed to the xW+b part, which is linear.) Why do we need to add non-linearities? Because if not, the entire network could collapse to one layer.\n",
    "\n",
    "Activation just means output. The linear activation in the last layer of this model means ‘return the output without doing anything more (like ReLU) to it’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer 1 with ReLU activation\n",
    "layer_1 = tf.add(tf.matmul(X_input, w1), b1)\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "# Hidden layer 2 with ReLU activation\n",
    "layer_2 = tf.add(tf.matmul(layer_1, w2), b2)\n",
    "layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "# Output layer with linear activation\n",
    "out_layer = tf.matmul(layer_2, out_w ) + out_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.losses.mean_squared_error(y_label,out_layer)\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(0.001).minimize(losses) #using Adaptive Moment Estimation (Adam)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.09).minimize(losses) #using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************weights before training*********************************\n",
      "                                                                                \n",
      "weight_1: \n",
      "[[ 0.19714087 -0.53576887 -1.6313076  -0.46538144 -0.24096622]\n",
      " [ 0.48001674  1.38334513  2.1973803  -1.50088358 -1.11959493]\n",
      " [-0.55318099  0.07201028 -2.67183828 -0.3964572   0.20036089]\n",
      " [-1.26047742  0.0318081  -1.26312864 -0.6359393   0.36029524]\n",
      " [-0.00574145  0.58239913 -1.10277545  0.13962074 -0.33975729]\n",
      " [-0.8128832  -0.33963752  0.03247813 -0.92922157 -0.29505524]\n",
      " [ 0.24585201 -0.51315165 -0.45331183 -0.88602358  0.20855016]\n",
      " [ 0.74723089 -1.01083338 -0.21508068  0.21001558 -1.73101401]]\n",
      "weight_2: \n",
      "[[-1.63071394 -0.42717698 -2.45502782]\n",
      " [ 0.65831822 -0.1745652  -1.46276009]\n",
      " [ 0.02916694  0.36403146 -0.07637378]\n",
      " [ 1.35566914  0.5127328  -1.04606068]\n",
      " [-0.37508479  0.15286568 -1.00266933]]\n",
      "weight_3: \n",
      "[[ 1.3991344   0.16665764]\n",
      " [-0.28473514  1.08020461]\n",
      " [ 0.57035786  0.02002707]]\n",
      "-------------------------Training Result------------------------------------------\n",
      "                                                                                 \n",
      "losses after per 10000 iteration:  2.20562e+06\n",
      "losses after per 10000 iteration:  0.128073\n",
      "losses after per 10000 iteration:  0.128073\n",
      "losses after per 10000 iteration:  0.128073\n",
      "losses after per 10000 iteration:  0.128073\n",
      "                                                                                         \n",
      "-------------------------------------Accuracy--------------------------------------------\n",
      "                                                                                         \n",
      "Accuracy on the model:  0.84918\n",
      "***************************** weight after training *************************************\n",
      "weight_1 after trainig: \n",
      "[[  1.97140872e-01  -5.82006111e+13  -1.63130760e+00  -4.65381444e-01\n",
      "   -8.07121484e+04]\n",
      " [  4.80016738e-01  -1.64215538e+11   2.19738030e+00  -1.50088358e+00\n",
      "   -2.17380508e+02]\n",
      " [ -5.53180993e-01  -2.30056460e+15  -2.67183828e+00  -3.96457195e-01\n",
      "   -3.00703300e+06]\n",
      " [ -1.26047742e+00  -9.24775547e+11  -1.26312864e+00  -6.35939300e-01\n",
      "   -1.21187671e+03]\n",
      " [ -5.74145140e-03  -3.05370418e+12  -1.10277545e+00   1.39620736e-01\n",
      "   -4.22138086e+03]\n",
      " [ -8.12883198e-01  -2.42511299e+11   3.24781276e-02  -9.29221570e-01\n",
      "   -3.20304260e+02]\n",
      " [  2.45852008e-01  -1.59836673e+13  -4.53311831e-01  -8.86023581e-01\n",
      "   -2.09254219e+04]\n",
      " [  7.47230887e-01  -2.13444977e+10  -2.15080678e-01   2.10015580e-01\n",
      "   -3.02399921e+01]]\n",
      "weight_2 after training: \n",
      "[[ -1.63071394e+00  -4.27176982e-01  -2.45502782e+00]\n",
      " [ -1.19185738e+22  -1.08195575e+06  -1.46276009e+00]\n",
      " [  2.91669443e-02   3.64031464e-01  -7.63737783e-02]\n",
      " [  1.35566914e+00   5.12732804e-01  -1.04606068e+00]\n",
      " [ -3.75084788e-01  -3.83358775e+06  -1.00266933e+00]]\n",
      "weight_3 after training: \n",
      "[[ -5.52945151e+21  -6.58639553e+20]\n",
      " [  9.06658906e+04  -3.43763562e+05]\n",
      " [  5.70357859e-01   2.00270712e-02]]\n",
      "-------------------------------------Results--------------------------------------------\n",
      "                                                                                         \n",
      "[0.15081961, 0.84918004]\n",
      "WRITE OFF\n"
     ]
    }
   ],
   "source": [
    "# Initiate the session and setting global variables\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"************************weights before training*********************************\"\n",
    "print \"                                                                                \"\n",
    "\n",
    "\n",
    "we1,we2,we3 = sess.run([w1,w2,out_w])\n",
    "print \"weight_1: \"\n",
    "print we1\n",
    "print \"weight_2: \"\n",
    "print we2\n",
    "print \"weight_3: \"\n",
    "print we3\n",
    "\n",
    "print \"-------------------------Training Result------------------------------------------\"\n",
    "print \"                                                                                 \"\n",
    "\n",
    "step_size = 10000\n",
    "for step in range(step_size):\n",
    "    \n",
    "    a,b,c,d,e,f,g,h = sess.run([layer_1,layer_2,out_layer,losses,optimizer,w1,w2,out_w], feed_dict={X_input:x_labels, y_output:y_label})\n",
    "\n",
    "    if step%2000==0:\n",
    "        print \"losses after per 10000 iteration: \",d\n",
    "\n",
    "print \"                                                                                         \"       \n",
    "correct_prediction = tf.equal(tf.argmax(out_layer,1), tf.argmax(y_output,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print \"-------------------------------------Accuracy--------------------------------------------\"\n",
    "print \"                                                                                         \"\n",
    "\n",
    "print \"Accuracy on the model: \",accuracy.eval(feed_dict={X_input:x_labels, y_output:y_label})\n",
    "\n",
    "#check the model with the first observation\n",
    "check_1 = tf.add(tf.matmul([[755.16, 3, 44212, 46, 60.41, 5, 651, 0]], w1), b1)\n",
    "check_1 = tf.nn.relu(check_1)\n",
    "    \n",
    "check_2 = tf.add(tf.matmul(check_1, w2), b2)\n",
    "check_2 = tf.nn.relu(check_2)\n",
    "\n",
    "output = tf.nn.relu(tf.matmul(check_2, out_w) + out_b)\n",
    "\n",
    "print \"***************************** weight after training *************************************\"\n",
    "print \"weight_1 after trainig: \"\n",
    "print f\n",
    "print \"weight_2 after training: \"\n",
    "print g\n",
    "print \"weight_3 after training: \"\n",
    "print h\n",
    "\n",
    "a,b,c = sess.run([check_1, check_2 ,output])\n",
    "\n",
    "print \"-------------------------------------Results--------------------------------------------\"\n",
    "print \"                                                                                         \"\n",
    "\n",
    "convert_list = list(itertools.chain.from_iterable(c))\n",
    "print convert_list\n",
    "\n",
    "indx = sess.run(tf.argmax(convert_list))\n",
    "\n",
    "if(indx==0):\n",
    "    print \"NO WRITE OFF\"\n",
    "elif(indx==1):\n",
    "    print \"WRITE OFF\"\n",
    "else:\n",
    "    print \"WHAT??\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This illustrative python notebook shows how to get started with basic deep learning utilizing TensorFlow library and MLP technique. I hope you to see how easy to adopt IBM's Data Science Experience for your data analytics and modeling needs. Please find overview and getting-started information in the Data Science Experience documentation: https://datascience.ibm.com/docs/content/getting-started/welcome-main.html. Learn about Jupyter notebooks, which are used throughout this scenario, in the Data Science Experience documentation: https://datascience.ibm.com/docs/content/analyze-data/notebooks-parent.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6 (Unsupported)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
