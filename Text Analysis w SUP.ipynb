{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JCL (Just Copy the Lines!!!)\n",
    "\n",
    "Actually, JCL stands for Job Control Language (in TSO mainframe environment), but it was a great piece of advice that I heard half-jokingly back in 20 somewhat years ago, when I just started my career.  I had to learn SAS and SQL as a part of my job analyzing credit risks in a risk management division.   I certainly didn’t have a career aspiration to be a programmer, but it helped me a great deal to understand the underlying aspects of the business process that I had to manage.  We all have seen many innovations started off from mere imitations, and I think it still is a good way to learn new stuff at an individual level.  Well, learning and understanding machine learning wouldn’t be an exception to this.\n",
    "\n",
    "## Unstructured data and a real busines problem\n",
    "Approximately, 90% of the data is generated for the last two years, of which 85% of the data is unstructured.  For those who can unleash the powerful insights from the unstructured data, they will, no doubt, create a superior competitive advantage.   In this notebook, I chose a sample hypothetical SMS data to predict churn response by training Naive Bayes and Logistic Regression algorithms as the dependent variable is categorical.  In addition, I have included some basic and conceptual way to understand how text analysis works, which I think is interesting before running machine learnings to train models. This illustrative exercise is a very basic form of Natural Language Processing (NLP), and I hope you can understand the basics and an illustrative example that can be applicable for a real business problem.\n",
    "\n",
    "> Natural Language Processing (NLP): NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.  \n",
    "\n",
    "> Although NLP could get very complex, in a nutshell, the data (text or speech) needs to be \"parameterized\" (or making the data to \"numbers\") so that machine learning algorithms can kicks in and do the magics :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with SMS Text Analysis and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Basics\n",
    "2. Supervised ML: Text analysis with sample dummy data: Vectorizing & modeling\n",
    "3. Comparing with the other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for Python 2: use print only as a function\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Representing text as numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example text for model training (SMS messages)\n",
    "simple_train = ['please cancel my policy', 'renew... happy with service', 'fraud on my policy', 'renew my auto policy', 'service was great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example response vector\n",
    "likely_churn = [1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'auto',\n",
       " u'cancel',\n",
       " u'fraud',\n",
       " u'great',\n",
       " u'happy',\n",
       " u'my',\n",
       " u'on',\n",
       " u'please',\n",
       " u'policy',\n",
       " u'renew',\n",
       " u'service',\n",
       " u'was',\n",
       " u'with']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x13 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auto</th>\n",
       "      <th>cancel</th>\n",
       "      <th>fraud</th>\n",
       "      <th>great</th>\n",
       "      <th>happy</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>please</th>\n",
       "      <th>policy</th>\n",
       "      <th>renew</th>\n",
       "      <th>service</th>\n",
       "      <th>was</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auto  cancel  fraud  great  happy  my  on  please  policy  renew  service  \\\n",
       "0     0       1      0      0      0   1   0       1       1      0        0   \n",
       "1     0       0      0      0      1   0   0       0       0      1        1   \n",
       "2     0       0      1      0      0   1   1       0       1      0        0   \n",
       "3     1       0      0      0      0   1   0       0       1      1        0   \n",
       "4     0       0      0      1      0   0   0       0       0      0        1   \n",
       "\n",
       "   was  with  \n",
       "0    0     0  \n",
       "1    0     1  \n",
       "2    0     0  \n",
       "3    0     0  \n",
       "4    1     0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "type(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 9)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a model to predict desperation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(simple_train_dtm, likely_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"renew my policy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auto</th>\n",
       "      <th>cancel</th>\n",
       "      <th>fraud</th>\n",
       "      <th>great</th>\n",
       "      <th>happy</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>please</th>\n",
       "      <th>policy</th>\n",
       "      <th>renew</th>\n",
       "      <th>service</th>\n",
       "      <th>was</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auto  cancel  fraud  great  happy  my  on  please  policy  renew  service  \\\n",
       "0     0       0      0      0      0   1   0       0       1      1        0   \n",
       "\n",
       "   was  with  \n",
       "0    0     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict whether simple_test likely to churn\n",
    "knn.predict(simple_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` **learns the vocabulary** of the training data\n",
    "- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n",
    "- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a sample dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/YLEE200/MLFS/master/testdata/churnSMS.csv'\n",
    "\n",
    "sms = pd.read_table(url, header=None, sep = \",\", names=['label', 'message'], encoding = 'iso-8859-1')\n",
    "\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datactr1</td>\n",
       "      <td>cofidential and proprietary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>churn</td>\n",
       "      <td>this is interestingÃ please cancel my policy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>churn</td>\n",
       "      <td>would you call me ASAP?  My number is 123-555-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>churn</td>\n",
       "      <td>I am not renewing my policy. This is sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>churn</td>\n",
       "      <td>there has been a fraud on my account. please c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>churn</td>\n",
       "      <td>OMG, I don't like thisÃ. I am done w/ my account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>churn</td>\n",
       "      <td>FRAUD, for god sake.. What is going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>churn</td>\n",
       "      <td>Too expensive!  There should be some discountÃ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>churn</td>\n",
       "      <td>My friend told me, thereÃs a better deal out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>churn</td>\n",
       "      <td>Can you believe it? Cancel my account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message\n",
       "0  datactr1                        cofidential and proprietary\n",
       "1     churn  this is interestingÃ please cancel my policy ...\n",
       "2     churn  would you call me ASAP?  My number is 123-555-...\n",
       "3     churn         I am not renewing my policy. This is sucks\n",
       "4     churn  there has been a fraud on my account. please c...\n",
       "5     churn  OMG, I don't like thisÃ. I am done w/ my account\n",
       "6     churn            FRAUD, for god sake.. What is going on?\n",
       "7     churn   Too expensive!  There should be some discountÃ.\n",
       "8     churn  My friend told me, thereÃs a better deal out ...\n",
       "9     churn              Can you believe it? Cancel my account"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 10 rows\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retain      217\n",
       "churn       185\n",
       "datactr1      1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>churn</td>\n",
       "      <td>this is interestingÃ please cancel my policy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>churn</td>\n",
       "      <td>would you call me ASAP?  My number is 123-555-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>churn</td>\n",
       "      <td>I am not renewing my policy. This is sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>churn</td>\n",
       "      <td>there has been a fraud on my account. please c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>churn</td>\n",
       "      <td>OMG, I don't like thisÃ. I am done w/ my account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>churn</td>\n",
       "      <td>FRAUD, for god sake.. What is going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>churn</td>\n",
       "      <td>Too expensive!  There should be some discountÃ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>churn</td>\n",
       "      <td>My friend told me, thereÃs a better deal out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>churn</td>\n",
       "      <td>Can you believe it? Cancel my account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>churn</td>\n",
       "      <td>Totally unhappy.. How can I close my policy?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                            message\n",
       "1   churn  this is interestingÃ please cancel my policy ...\n",
       "2   churn  would you call me ASAP?  My number is 123-555-...\n",
       "3   churn         I am not renewing my policy. This is sucks\n",
       "4   churn  there has been a fraud on my account. please c...\n",
       "5   churn  OMG, I don't like thisÃ. I am done w/ my account\n",
       "6   churn            FRAUD, for god sake.. What is going on?\n",
       "7   churn   Too expensive!  There should be some discountÃ.\n",
       "8   churn  My friend told me, thereÃs a better deal out ...\n",
       "9   churn              Can you believe it? Cancel my account\n",
       "10  churn       Totally unhappy.. How can I close my policy?"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging the first row\n",
    "sms = sms.loc[1:,:]\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'retain':0, 'churn':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>churn</td>\n",
       "      <td>I have enough.. cancelling my policy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>churn</td>\n",
       "      <td>what???</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>retain</td>\n",
       "      <td>I am happy with your service, please renew my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>retain</td>\n",
       "      <td>Auto-renew my policyÃ.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>retain</td>\n",
       "      <td>Auto-renew my policy, please!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>retain</td>\n",
       "      <td>Jack, what a guy!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>retain</td>\n",
       "      <td>Would you renew my policy?  My policy number i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>retain</td>\n",
       "      <td>Can I renew my policy for another two years?  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>retain</td>\n",
       "      <td>renew my accountÃ. My name is Jane Doe and ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>churn</td>\n",
       "      <td>I am through, you guys..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message  label_num\n",
       "393   churn               I have enough.. cancelling my policy          1\n",
       "394   churn                                            what???          1\n",
       "395  retain  I am happy with your service, please renew my ...          0\n",
       "396  retain                            Auto-renew my policyÃ.          0\n",
       "397  retain                     Auto-renew my policy, please!!          0\n",
       "398  retain                                Jack, what a guy!!!          0\n",
       "399  retain  Would you renew my policy?  My policy number i...          0\n",
       "400  retain  Can I renew my policy for another two years?  ...          0\n",
       "401  retain  renew my accountÃ. My name is Jane Doe and ph...          0\n",
       "402   churn                          I am through, you guys..           1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the conversion worked\n",
    "sms.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402,)\n",
      "(402,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.message\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301,)\n",
      "(101,)\n",
      "(301,)\n",
      "(101,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<301x170 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 2188 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<101x170 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 703 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and evaluating a model\n",
    "\n",
    "We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94059405940594054"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  5],\n",
       "       [ 1, 39]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269                                    can this be true?\n",
       "398                                  Jack, what a guy!!!\n",
       "262    please confirm my letter sent to your company ...\n",
       "275    It's a beautiful dayÃ is there someone that I...\n",
       "108                               is ther a better deal?\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives (retained account incorrectly classified as churned)\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327    help, help!!!\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false negatives (churn incorrectly classified as retain)\n",
    "X_test[y_test > y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'My friend told me, there\\xc3\\x95s a better deal out there.. please cancel my policy\\xc3\\x89.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example false negative\n",
    "X_test[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.60085743e-01,   4.59513095e-08,   8.10146636e-10,\n",
       "         3.99051681e-08,   2.17085512e-06,   9.99989741e-01,\n",
       "         9.99999980e-01,   6.38488959e-01,   9.99996914e-01,\n",
       "         1.47965194e-03,   1.45837640e-08,   9.98674643e-01,\n",
       "         8.10146636e-10,   1.11046421e-06,   9.89444357e-01,\n",
       "         9.99989741e-01,   9.99999998e-01,   9.99996914e-01,\n",
       "         1.46881066e-03,   8.76773822e-05,   3.11209192e-05,\n",
       "         9.79011147e-01,   1.46881066e-03,   9.96541569e-01,\n",
       "         9.52447297e-01,   9.99998142e-01,   9.99581812e-01,\n",
       "         1.62671848e-06,   2.17085512e-06,   1.03743894e-05,\n",
       "         1.11046421e-06,   9.99804038e-01,   9.96541569e-01,\n",
       "         9.99470776e-01,   9.99998142e-01,   4.59513095e-08,\n",
       "         1.98153632e-05,   1.45837640e-08,   9.99470776e-01,\n",
       "         1.47965194e-03,   9.99804038e-01,   9.99958399e-01,\n",
       "         3.52424789e-02,   6.45486467e-01,   2.17085512e-06,\n",
       "         1.03743894e-05,   3.52424789e-02,   1.11046421e-06,\n",
       "         2.86427349e-12,   8.71044113e-01,   9.95096327e-01,\n",
       "         1.03743894e-05,   4.81727575e-01,   1.03743894e-05,\n",
       "         9.78419877e-01,   9.99326187e-01,   9.99972274e-01,\n",
       "         9.99590525e-01,   8.02230331e-11,   8.76773822e-05,\n",
       "         6.89556443e-05,   1.62671848e-06,   6.89556443e-05,\n",
       "         1.11046421e-06,   3.11209192e-05,   8.02230331e-11,\n",
       "         9.99972274e-01,   1.00000000e+00,   1.00000000e+00,\n",
       "         8.76773822e-05,   8.02230331e-11,   7.79528692e-01,\n",
       "         3.52424789e-02,   3.69208027e-04,   1.45837640e-08,\n",
       "         7.83364150e-02,   9.99999864e-01,   1.98153632e-05,\n",
       "         9.90911059e-01,   9.99999864e-01,   8.10146636e-10,\n",
       "         9.99999999e-01,   1.47965194e-03,   9.90911059e-01,\n",
       "         2.86427349e-12,   3.69208027e-04,   3.52424789e-02,\n",
       "         1.03743894e-05,   1.47965194e-03,   9.99287544e-01,\n",
       "         4.59513095e-08,   9.99287544e-01,   1.46881066e-03,\n",
       "         1.00000000e+00,   9.84693966e-01,   9.99470776e-01,\n",
       "         3.99051681e-08,   1.96442910e-03,   7.79528692e-01,\n",
       "         8.73987863e-03,   9.99993886e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# calculate AUC\n",
    "#print (\"AUC is:\", metrics.roc_auc_score(y_test, y_pred_prob))\n",
    "print (\"AUC is: %.4f\" %(metrics.roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "We will compare multinomial Naive Bayes with [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression):\n",
    "\n",
    "> Logistic regression, despite its name, is a **linear model for classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81760288,  0.00548847,  0.01628215,  0.02714258,  0.01336658,\n",
       "        0.97342347,  0.97557232,  0.46024868,  0.93322274,  0.02744867,\n",
       "        0.00903146,  0.93191374,  0.01628215,  0.02410099,  0.90198242,\n",
       "        0.97342347,  0.99505563,  0.93322274,  0.07073719,  0.02096489,\n",
       "        0.06217511,  0.93688506,  0.07073719,  0.96839213,  0.89435956,\n",
       "        0.93668176,  0.8949201 ,  0.00604791,  0.01336658,  0.01481586,\n",
       "        0.02410099,  0.94269733,  0.96839213,  0.96618504,  0.93668176,\n",
       "        0.00548847,  0.0016576 ,  0.00903146,  0.96618504,  0.02744867,\n",
       "        0.94269733,  0.97616619,  0.07036081,  0.74444413,  0.01336658,\n",
       "        0.01481586,  0.07036081,  0.02410099,  0.00656223,  0.67398781,\n",
       "        0.95587561,  0.01481586,  0.60636782,  0.01481586,  0.68156671,\n",
       "        0.87306688,  0.97233238,  0.91269462,  0.00933831,  0.02096489,\n",
       "        0.01487404,  0.00604791,  0.01487404,  0.02410099,  0.06217511,\n",
       "        0.00933831,  0.97233238,  0.9879903 ,  0.9879903 ,  0.02096489,\n",
       "        0.00933831,  0.82293114,  0.07036081,  0.10528033,  0.00903146,\n",
       "        0.82183916,  0.9784478 ,  0.0016576 ,  0.94775692,  0.9784478 ,\n",
       "        0.01628215,  0.9951419 ,  0.02744867,  0.94775692,  0.00656223,\n",
       "        0.10528033,  0.07036081,  0.01481586,  0.02744867,  0.95406187,\n",
       "        0.00548847,  0.95406187,  0.07073719,  0.9879903 ,  0.81739544,\n",
       "        0.96618504,  0.02714258,  0.46090983,  0.82293114,  0.33809218,\n",
       "        0.98883181])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940594059406\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# calculate AUC\n",
    "print ('AUC is: %.4f' %(metrics.roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "\n",
    "This illustrative python notebook shows how to run simple machine learning techniques to analyze unstructured data (in this case, SMS text).  I hope you to see how easy to adopt Text Analysis for your data analytics and modeling needs.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
